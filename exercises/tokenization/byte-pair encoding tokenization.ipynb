{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Tokenization Using the Byte-Pair Encoding Algorithm  \n",
    "  \n",
    "The tokenization process consists of the following steps:  \n",
    "1. Normalization: In this step, we normalize the text to a standard format. For example, this includes converting all characters to ASCII, removing diacritics, etc.  \n",
    "2. Pre-tokenization: This step involves splitting the text into word tokens, typically by dividing the text at spaces and newlines.  \n",
    "3. Subword tokenization: In this phase, we break down the word tokens into smaller subword tokens. To accomplish this, we must first develop a statistical model that informs us of the most effective method for subdivision. The Byte-Pair Encoding algorithm is one such model.  \n",
    "  \n",
    "<img src=\"../../images/tokenization_process.jpg\" width=\"700\" />  \n",
    "  \n",
    "In this tutorial, we will initially explore the normalization and pre-tokenization steps. Subsequently, we will delve into the Byte-Pair Encoding algorithm and apply it to devise a subword tokenization model.  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5013540c9986bb24"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import collections\n",
    "from tqdm import tqdm  \n",
    "import unicodedata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:48:06.828258Z",
     "start_time": "2024-01-29T15:48:06.814628Z"
    }
   },
   "id": "87c8056ad656afa9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization\n",
    "Here we define a function that normalizes a Unicode strings to ASCII. It works as follows:\n",
    "1. Normalize the Unicode string to NFD (Normalization Form Decomposed) 'NFD' breaks down each character into its base character and diacritics. For example, the character `\u00e9` is decomposed into `e` and `\u00b4`.\n",
    "2. Filter out non-ASCII characters (i.e., characters with diacritics). For example, the character `\u00b4` is filtered out."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55b2ba51e840016c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    # Normalize the Unicode string to NFD (Normalization Form Decomposed) \n",
    "    # 'NFD' breaks down each character into its base character and diacritics (also known as \"combining marks\")\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    # Filter out non-ASCII characters (i.e., characters with diacritics)  \n",
    "    text = ''.join(char for char in text if _is_ascii(char)) \n",
    "    return text\n",
    "\n",
    "def _is_ascii(char):\n",
    "    return unicodedata.category(char) != 'Mn'\n",
    "\n",
    "result = normalize(\"H\u00e9ll\u00f2 h\u00f4w are \u00fc?\")\n",
    "expected = \"Hello how are u?\"\n",
    "assert result == expected, f\"Expected `{expected}`, got `{result}`\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T14:48:16.647774Z",
     "start_time": "2024-01-29T14:48:16.601514Z"
    }
   },
   "id": "afef7813a77e2fc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-tokenization\n",
    "In this step, we split the text into word and whitespace tokens. For example, `Hello how are u?` will be split into `[\"Hello\", \" \", \"how\", \" \", \"are\", \" \", \"u\", \"?\"]`. We keep still keep the `\" \"` and `\"\\n\"` tokens so that we can later revere the tokenization process.\n",
    "\n",
    "Complete the `pre_tokenize` function below by implementing the missing if-statements.\n",
    "When you are done, it should pass the tests below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "924b6af85199252e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pre_tokenize(text: str):\n",
    "    word_tokens = []\n",
    "    current_word = \"\"\n",
    "    for char in text:\n",
    "        if char == \" \":\n",
    "            # YOUR CODE HERE START\n",
    "            # YOUR CODE HERE END\n",
    "        elif char == \"\\n\":\n",
    "            # YOUR CODE HERE START\n",
    "            # YOUR CODE HERE END\n",
    "        else:\n",
    "            current_word += char\n",
    "\n",
    "    \n",
    "    if len(current_word) > 0:\n",
    "        word_tokens.append(current_word)\n",
    "\n",
    "    return word_tokens\n",
    "\n",
    "def un_pre_tokenize(tokens):\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "input_text = \"3.2.1: Let's  get started!\\nMy name is bob_smith\"\n",
    "assert pre_tokenize(input_text) == [\"3.2.1:\", \" \", \"Let's\", \" \", \" \", \"get\", \" \", \"started!\", \"\\n\", \"My\", \" \", \"name\",\" \", \"is\", \" \", \"bob_smith\"], f\"Got `{pre_tokenize(input_text)}`\"\n",
    "assert un_pre_tokenize(pre_tokenize(input_text)) == input_text, f\"Got `{un_pre_tokenize(pre_tokenize(input_text))}`\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T14:30:06.727696Z",
     "start_time": "2024-01-29T14:30:06.691075Z"
    }
   },
   "id": "e24f955723f999e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Byte-Pair Encoding  \n",
    "Byte-Pair Encoding (BPE) was originally devised as a text compression algorithm and later adopted by OpenAI for tokenization in the pretraining of the GPT model. \n",
    "\n",
    "\n",
    "The tokenization process is as follows:  \n",
    "1. Normalize the text and split it into word tokens.  \n",
    "2. Split each word token into a list of characters.  \n",
    "3. Sequentially apply the merge rules as they were discovered during the learning process to the subword tokens.  \n",
    "4. Concatenate and flatten the array of subword lists into a unified list of subword tokens.\n",
    "\n",
    "As you can see, the tokenization process need to learn two things from a corpus of text:\n",
    "- The initial vocabulary of characters: This learned by collecting all unique characters found in the corpus.\n",
    "- The merge rules: These rules define how we merge these characters into larger tokens. \n",
    "  \n",
    "The learning process includes the following steps:  \n",
    "1. Normalize the text and split it into word tokens.  \n",
    "2. Count the frequency of each word token. This allows us to retain only one copy of each word token in memory.  \n",
    "3. Construct an initial vocabulary consisting of all unique characters found in the word tokens.  \n",
    "4. Decompose each word token into the subwords available in the vocabulary. For instance, if the word token is `hello` and the vocabulary contains `{\"h\", \"e\", \"l\", \"o\"}`, then the word token will be split into `[\"h\", \"e\", \"l\", \"l\", \"o\"]`.  \n",
    "5. Calculate the frequency with which each adjacent pair of subword tokens occurs, considering the frequency of the word tokens. For example, if the word token `hello` appears twice, then the pair `(\"l\", \"l\")` will be counted twice.  \n",
    "6. Identify the most frequently occurring pair of subwords and merge them into a single subword token. For example, if the pair `(\"l\", \"l\")` is most frequent, it will be merged to form the subword token `(\"ll\")`. Incorporate this new subword token into the vocabulary and add the corresponding pair to the merge rules.  \n",
    "7. Apply this new merge rule to all subword tokens. For instance, if the subword token is `[\"h\", \"e\", \"l\", \"l\", \"o\"]` and the merge rule is `(\"l\", \"l\") -> (\"ll\")`, then the subword token will be transformed into `[\"h\", \"e\", \"ll\", \"o\"]`. Ensure to keep track of the sequence in which the merge rules were identified.  \n",
    "8. Repeat steps 4-6 until the vocabulary attains the desired size.  \n",
    "  \n",
    "  \n",
    "Before we begin implementing the algorithm, we will first define some helper functions.  \n",
    "Let's start by creating the count_frequency function. This function accepts a list of word tokens as input and returns the frequency of each unique word token found within the list. For example, if the input is `[\"hello\", \"hello\", \"world\"]`, then the output would be `{\"hello\": 2, \"world\": 1}`.  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8391ce465fec1587"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def count_frequency(items: list[str]):\n",
    "    \"\"\"\n",
    "    :param items: List of word tokens \n",
    "    :return: Dictionary mapping each unique word token to its frequency\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "\n",
    "text = \"3.2.1: Let's get started!\"\n",
    "token_freq = count_frequency(pre_tokenize(text))\n",
    "assert token_freq == {\"3.2.1:\": 1, \"Let's\": 1, \"get\": 1, \"started!\": 1, \" \": 3}, f\"Got `{token_freq}`\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:48:16.632370Z",
     "start_time": "2024-01-29T15:48:16.597237Z"
    }
   },
   "id": "39f6943fd9499f6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To determine the merge rules, we need to identify which pairs of tokens occur most frequently together in the corpus. To accomplish this, we utilize the following helper function. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c4c4a23a1169701"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def count_pair_frequencies(\n",
    "    subtoken_per_word_token: dict[str, list[str]],\n",
    "    word_token_frequency: dict[str, int]\n",
    "):\n",
    "    \"\"\"\n",
    "    :param subtoken_per_word_token: This the current mapping of each word token to its subtokens. E.g. if the word \"going\" is split into [\"go\", \"ing\"], then subtoken_per_word_token[\"going\"] == [\"go\", \"ing\"]\n",
    "    :param word_token_frequency: A dictionary mapping each unique word token to its frequency in the corpus. E.g. if the word \"going\" appears 314 times in the corpus, then word_token_frequency[\"going\"] == 314\n",
    "    :return: A dictionary mapping each pair of subtokens to its frequency in the corpus. E.g. if the pair (\"go\", \"ing\") appears 314 times in the corpus, then the output would be {(\"go\", \"ing\"): 314}\n",
    "    \"\"\"\n",
    "    pair_frequencies = collections.defaultdict(lambda: 0)\n",
    "    \n",
    "    for token, freq in word_token_frequency.items():\n",
    "        subtokens = subtoken_per_word_token[token]\n",
    "        for i in range(len(subtokens) - 1):\n",
    "            pair = (subtokens[i], subtokens[i + 1])\n",
    "            pair_frequencies[pair] += freq\n",
    "   \n",
    "    return pair_frequencies\n",
    "\n",
    "subtoken_per_word_token={\"going\": [\"go\", \"i\", \"ng\"], \"boing\": [\"bo\", \"i\", \"ng\"]}\n",
    "word_token_frequency={\"going\": 314, \"boing\": 42}\n",
    "pair_frequencies = count_pair_frequencies(subtoken_per_word_token, word_token_frequency)\n",
    "assert pair_frequencies[(\"go\", \"i\")] == 314, f\"Got `{pair_frequencies[('go', 'i')]}`\"\n",
    "assert pair_frequencies[(\"i\", \"ng\")] == 356, f\"Got `{pair_frequencies[('i', 'ng')]}`\"\n",
    "assert pair_frequencies[(\"bo\", \"i\")] == 42, f\"Got `{pair_frequencies[('bo', 'i')]}`\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:48:17.484365Z",
     "start_time": "2024-01-29T15:48:17.455068Z"
    }
   },
   "id": "80be86c351d0296c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need an `apply_merge_rule` that merges each `token_a` and `token_b` that appear next to each other in the `subtokens` list. For example, if `token_a` is `\"l\"` and `token_b` is `\"l\"`, then the function should return a new list where all occurrences of `[\"l\", \"l\"]` are replaced with `[\"ll\"]`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f97b63dadb1a20"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def apply_merge_rule(token_a: str, token_b: str, subtokens: list[str]):\n",
    "    new_subtokens = []  \n",
    "  \n",
    "    # We cannot do a for because we either take 1 step if we don't merge or 2 steps if we do merge.  \n",
    "    i = 0  \n",
    "    while i < len(subtokens): \n",
    "        # You have to handle the following cases:\n",
    "        # 1. if token_i == token_a and token_i+1 == token_b, then merge the two tokens\n",
    "        # 2. if token_i != token_a or token_i+1 != token_b, then keep the token as is\n",
    "        # YOUR CODE HERE START:\n",
    "        # YOUR CODE HERE END\n",
    "        \n",
    "\n",
    "    return new_subtokens\n",
    "\n",
    "\n",
    "token_a = \"l\"\n",
    "token_b = \"l\"\n",
    "subtokens = [\"h\", \"e\", \"l\", \"l\", \"o\"]\n",
    "assert apply_merge_rule(token_a, token_b, subtokens) == [\"h\", \"e\", \"ll\", \"o\"], f\"Got `{apply_merge_rule(token_a, token_b, subtokens)}`\"\n",
    "\n",
    "token_a = \"ll\"\n",
    "token_b = \"o\"\n",
    "subtokens = [\"h\", \"e\", \"ll\", \"o\"]\n",
    "assert apply_merge_rule(token_a, token_b, subtokens) == [\"h\", \"e\", \"llo\"], f\"Got `{apply_merge_rule(token_a, token_b, subtokens)}`\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:48:18.049585Z",
     "start_time": "2024-01-29T15:48:18.000827Z"
    }
   },
   "id": "983c65e12ae59806"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning Merge rules\n",
    "Now that we have defined all the helper functions, we can start implementing the learning process.\n",
    "Most of the code is already written for you. You only need to implement the missing parts which are indicated by the `# YOUR CODE HERE START` and `# YOUR CODE HERE END` comments."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88b0f6d75e9593a0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_vocab_size: int, \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param max_vocab_size: The maximum size of the vocabulary. \n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "        self._merge_rules: list[tuple[str, str]] = []\n",
    "        self.vocab: set[str] = set()\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit(self, text: str):\n",
    "        \"\"\"\n",
    "        Learn the merge rules from the given text corpus.\n",
    "        \n",
    "        :param text: The text corpus to learn the merge rules from.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # 1. Normalizing the text\n",
    "        text = normalize(text)\n",
    "        # 2. Pre-tokenizing the text\n",
    "        word_tokens = pre_tokenize(text)\n",
    "        # 3. Counting the frequency of each word token\n",
    "        word_token_frequency = count_frequency(word_tokens)\n",
    "        \n",
    "        \n",
    "        # 4. Constructing an initial vocabulary consisting of all unique characters found in the word tokens\n",
    "        unique_word_tokens = set(word_token_frequency.keys())\n",
    "        subtoken_per_word_token: dict[str, list[str]] = {}\n",
    "        for word_token in unique_word_tokens:\n",
    "            for char in word_token:\n",
    "                # Here we add each unique character to the initial vocabulary\n",
    "                self.vocab.add(char)\n",
    "            \n",
    "            # Here we decompose each word token into a list of characters\n",
    "            # This is the most basic subword token since no merge rules have been applied yet\n",
    "            subtoken_per_word_token[word_token] = list(word_token)\n",
    "            \n",
    "\n",
    "        with tqdm(total=self.max_vocab_size) as progress_bar:\n",
    "            \n",
    "            while len(self.vocab) < self.max_vocab_size:\n",
    "                # 5. Find the most frequent pair of subtokens in the corpus.\n",
    "                # Use the count_pair_frequencies function to define the pair_frequencies variable.\n",
    "                # YOUR CODE HERE START\n",
    "                # YOUR CODE HERE END\n",
    "                \n",
    "                if len(pair_frequencies) == 0:\n",
    "                    print(\"No more pairs to merge\")\n",
    "                    break\n",
    "                \n",
    "                # find the most frequent pair in the pair_frequencies dictionary and assign it to the most_frequent_pair variable\n",
    "                # YOUR CODE HERE START\n",
    "                # YOUR CODE HERE END\n",
    "                token_a, token_b = most_frequent_pair\n",
    "                \n",
    "                # 6. Add the merge rule to the list of merge rules and aplly it to all subtokens\n",
    "                self._merge_rules.append(most_frequent_pair)\n",
    "                self.vocab.add(token_a + token_b)\n",
    "                \n",
    "                for word, subtokens in subtoken_per_word_token.items():\n",
    "                    # we apply the merge rule to each subtoken list and replace it with the new list of subtokens\n",
    "                    subtoken_per_word_token[word] = apply_merge_rule(token_a, token_b, subtokens)\n",
    "                \n",
    "                progress_bar.update(len(self.vocab) - progress_bar.n)\n",
    "                \n",
    "\n",
    "    def encode(self, text: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Encode the given text into a list of subword tokens based on the learned merge rules.\n",
    "        :param text: The text to encode\n",
    "        :return: A list of subword tokens\n",
    "        \"\"\"\n",
    "        text = normalize(text)\n",
    "        word_tokens = pre_tokenize(text)\n",
    "        \n",
    "        result = []\n",
    "        for word_token in word_tokens:\n",
    "            subtokens = list(word_token)\n",
    "            for merge_rule in self._merge_rules:\n",
    "                # apply the merge rule to the subtokens using the apply_merge_rule function\n",
    "                # YOUR CODE HERE START\n",
    "                # YOUR CODE HERE END\n",
    "            \n",
    "            # We extend the result list with the subtokens of the current word token\n",
    "            # such that we get a flat list of subtokens\n",
    "            result.extend(subtokens)\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def decode(self, sub_tokens: list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Decode the given list of subword tokens into a string.\n",
    "        :param sub_tokens: The list of subword tokens to decode\n",
    "        :return: A string.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = \"\"\n",
    "\n",
    "        for sub_token in sub_tokens:\n",
    "            result += sub_token\n",
    "            \n",
    "    \n",
    "        \n",
    "        return result\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:48:18.716954Z",
     "start_time": "2024-01-29T15:48:18.682130Z"
    }
   },
   "id": "f184c199aa855d18"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [00:08<00:00, 31.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/shakespeare.txt\") as f:\n",
    "    shakespeare = f.read()\n",
    "    \n",
    "max_vocab_size = 250 # 250 is fast but increase it to large value for more realistic results\n",
    "bpe_tokenizer = BPETokenizer(max_vocab_size=max_vocab_size)\n",
    "bpe_tokenizer.fit(shakespeare)\n",
    "\n",
    "inputs = \"\"\"First Citizen:\n",
    "He's one honest enough: would all the rest were so!\"\"\"\n",
    "tokens = bpe_tokenizer.encode(inputs)\n",
    "assert isinstance(tokens, list), f\"Got `{type(tokens)}` instead of list\"\n",
    "assert all(isinstance(token, str) for token in tokens), f\"Got `{[type(token) for token in tokens]}` instead of list[str]\"\n",
    "assert bpe_tokenizer.decode(tokens) == inputs, f\"Got `{bpe_tokenizer.decode(tokens)}`\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:48:27.540393Z",
     "start_time": "2024-01-29T15:48:19.290049Z"
    }
   },
   "id": "d0ad4605c6d49dcd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 250\n",
      "- `:`\n",
      "- `I`\n",
      "- `i`\n",
      "- `,`\n",
      "- `W`\n",
      "- `F`\n",
      "- `T`\n",
      "- `K`\n",
      "- `G`\n",
      "- `g`\n",
      "- `h`\n",
      "- `r`\n",
      "- `3`\n",
      "- `R`\n",
      "- `z`\n",
      "- `n`\n",
      "- `k`\n",
      "- `u`\n",
      "- `\n",
      "`\n",
      "- `X`\n",
      "- `$`\n",
      "- `?`\n",
      "- ` `\n",
      "- `.`\n",
      "- `a`\n",
      "- `d`\n",
      "- `f`\n",
      "- `p`\n",
      "- `'`\n",
      "- `o`\n",
      "- `v`\n",
      "- `N`\n",
      "- `L`\n",
      "- `A`\n",
      "- `P`\n",
      "- `S`\n",
      "- `m`\n",
      "- `D`\n",
      "- `l`\n",
      "- `E`\n",
      "- `J`\n",
      "- `M`\n",
      "- `j`\n",
      "- `!`\n",
      "- `e`\n",
      "- `;`\n",
      "- `B`\n",
      "- `s`\n",
      "- `w`\n",
      "- `y`\n",
      "- `x`\n",
      "- `Z`\n",
      "- `b`\n",
      "- `U`\n",
      "- `&`\n",
      "- `C`\n",
      "- `t`\n",
      "- `c`\n",
      "- `V`\n",
      "- `Y`\n",
      "- `O`\n",
      "- `Q`\n",
      "- `q`\n",
      "- `H`\n",
      "- `-`\n",
      "- `or`\n",
      "- `le`\n",
      "- `te`\n",
      "- `oo`\n",
      "- `nd`\n",
      "- `it`\n",
      "- `po`\n",
      "- `AR`\n",
      "- `ha`\n",
      "- `su`\n",
      "- `do`\n",
      "- `li`\n",
      "- `we`\n",
      "- `ow`\n",
      "- `ch`\n",
      "- `'t`\n",
      "- `qu`\n",
      "- `A:`\n",
      "- `ra`\n",
      "- `fa`\n",
      "- `er`\n",
      "- `ne`\n",
      "- `is`\n",
      "- `ay`\n",
      "- `ld`\n",
      "- `pe`\n",
      "- `an`\n",
      "- `to`\n",
      "- `ta`\n",
      "- `me`\n",
      "- `'d`\n",
      "- `S:`\n",
      "- `am`\n",
      "- `ce`\n",
      "- `ar`\n",
      "- `so`\n",
      "- `un`\n",
      "- `ve`\n",
      "- `by`\n",
      "- `sh`\n",
      "- `gh`\n",
      "- `il`\n",
      "- `al`\n",
      "- `ca`\n",
      "- `up`\n",
      "- `UC`\n",
      "- `ad`\n",
      "- `ru`\n",
      "- `ma`\n",
      "- `ET`\n",
      "- `bo`\n",
      "- `ou`\n",
      "- `s,`\n",
      "- `th`\n",
      "- `se`\n",
      "- `ri`\n",
      "- `ur`\n",
      "- `mu`\n",
      "- `my`\n",
      "- `ke`\n",
      "- `e,`\n",
      "- `lo`\n",
      "- `EN`\n",
      "- `ge`\n",
      "- `st`\n",
      "- `en`\n",
      "- `ut`\n",
      "- `de`\n",
      "- `et`\n",
      "- `id`\n",
      "- `in`\n",
      "- `ir`\n",
      "- `mo`\n",
      "- `on`\n",
      "- `Th`\n",
      "- `ER`\n",
      "- `ck`\n",
      "- `pr`\n",
      "- `be`\n",
      "- `AN`\n",
      "- `IN`\n",
      "- `ro`\n",
      "- `'s`\n",
      "- `ho`\n",
      "- `at`\n",
      "- `as`\n",
      "- `re`\n",
      "- `fe`\n",
      "- `es`\n",
      "- `go`\n",
      "- `of`\n",
      "- `la`\n",
      "- `OR`\n",
      "- `wi`\n",
      "- `y,`\n",
      "- `ly`\n",
      "- `ti`\n",
      "- `om`\n",
      "- `he`\n",
      "- `To`\n",
      "- `O:`\n",
      "- `no`\n",
      "- `hi`\n",
      "- `ed`\n",
      "- `if`\n",
      "- `us`\n",
      "- `ll`\n",
      "- `our`\n",
      "- `are`\n",
      "- `she`\n",
      "- `ght`\n",
      "- `sha`\n",
      "- `mor`\n",
      "- `rom`\n",
      "- `ord`\n",
      "- `art`\n",
      "- `ain`\n",
      "- `ine`\n",
      "- `ING`\n",
      "- `ion`\n",
      "- `hat`\n",
      "- `ill`\n",
      "- `US:`\n",
      "- `ath`\n",
      "- `his`\n",
      "- `not`\n",
      "- `all`\n",
      "- `'ll`\n",
      "- `you`\n",
      "- `ver`\n",
      "- `ant`\n",
      "- `ard`\n",
      "- `end`\n",
      "- `man`\n",
      "- `The`\n",
      "- `ome`\n",
      "- `hen`\n",
      "- `and`\n",
      "- `IO:`\n",
      "- `the`\n",
      "- `ere`\n",
      "- `ent`\n",
      "- `con`\n",
      "- `ven`\n",
      "- `And`\n",
      "- `ter`\n",
      "- `But`\n",
      "- `one`\n",
      "- `for`\n",
      "- `him`\n",
      "- `ble`\n",
      "- `her`\n",
      "- `thy`\n",
      "- `now`\n",
      "- `own`\n",
      "- `ess`\n",
      "- `ous`\n",
      "- `sel`\n",
      "- `but`\n",
      "- `ood`\n",
      "- `was`\n",
      "- `ing`\n",
      "- `KING`\n",
      "- `thou`\n",
      "- `king`\n",
      "- `lord`\n",
      "- `thee`\n",
      "- `this`\n",
      "- `What`\n",
      "- `here`\n",
      "- `with`\n",
      "- `that`\n",
      "- `come`\n",
      "- `ther`\n",
      "- `hich`\n",
      "- `good`\n",
      "- `will`\n",
      "- `from`\n",
      "- `your`\n",
      "- `have`\n",
      "- `ight`\n",
      "- `what`\n",
      "- `ould`\n",
      "- `That`\n",
      "- `shall`\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {len(bpe_tokenizer.vocab)}\")\n",
    "for token in sorted(bpe_tokenizer.vocab, key=len):\n",
    "    print(f\"- `{token}`\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:49:10.534296Z",
     "start_time": "2024-01-29T15:49:10.523603Z"
    }
   },
   "id": "eb2a0df59930b59d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e218c34bef5ec218"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}